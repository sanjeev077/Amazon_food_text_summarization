{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_summarization_amazon_food.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "koiRlEktR08P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VFVu9BCwY1v",
        "colab_type": "code",
        "outputId": "eccdf08a-d90d-407c-8e43-5814229608d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
        "            if verbose:\n",
        "                print('wa.s>',W_a_dot_s.shape)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>',U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        def create_inital_state(inputs, hidden_size):\n",
        "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
        "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
        "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
        "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
        "            return fake_state\n",
        "\n",
        "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
        "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YMlFAF2wbBz",
        "colab_type": "code",
        "outputId": "3d101302-beb7-4942-b240-3848f2c21757",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input,LSTM,Embedding,Dense,Concatenate,TimeDistributed,Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings \n",
        "pd.set_option(\"display.max_colwidth\",200)\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SqAOoI2wd39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=pd.read_csv(\"../input/amazon-fine-food-reviews/Reviews.csv\",nrows=200000)\n",
        "#200000 over here is to only include 200000 raw data from total data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04PCoWyOwhVt",
        "colab_type": "code",
        "outputId": "47c80ba5-aa27-428d-c5e7-0c39947024af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "data[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9617292eb96a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QUSsKdYwjSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data preprocessing\n",
        "#1)drop duplicates and na\n",
        "# expanding contraction\n",
        "#2)stopword removal\n",
        "# 3)text cleaning ,removal of special character\n",
        "# 4)padding\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoqgxurAwl0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.drop_duplicates(subset=['Text'],inplace=True)\n",
        "data.dropna(axis=0,inplace =True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XME1a6W_wono",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creatin dictionary fro expanding contractions\n",
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "\n",
        "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DonZaB9gwqOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words=set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oJqEeSMwtUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT_CLEANING_RE=\"@\\S+|https?:\\S+|http?:\\S\\[^A-Za-z0-9]+\"\n",
        "def preprocessing(text):\n",
        "    newstring=text.lower()\n",
        "    newstring=BeautifulSoup(newstring,'lxml').text#to get text from html and xml pages\n",
        "    newstring=re.sub(TEXT_CLEANING_RE,'',newstring)\n",
        "    newstring = re.sub(r'\\([^)]*\\)', '', newstring)\n",
        "    newstring=re.sub('\"','',newstring)\n",
        "    newstring=re.sub(r\"'s\\b\",\"\",newstring)\n",
        "    newstring=re.sub(\"[^a-zA-Z]\", \" \", newstring)\n",
        "    word=[]\n",
        "    for i in newstring.split():\n",
        "        if i in contraction_mapping:\n",
        "            word.append(contraction_mapping[i])\n",
        "        else:\n",
        "            word.append(i)\n",
        "    \n",
        "    newstring=' '.join(word)\n",
        "    \n",
        "    wordb=[]\n",
        "    for i in newstring.split():\n",
        "        if i not in stop_words:\n",
        "            wordb.append(i)\n",
        "    \n",
        "    fword=[]\n",
        "    \n",
        "    for i in wordb:\n",
        "        if len(i)>=3:\n",
        "            fword.append(i)\n",
        "    \n",
        "    return \" \".join(fword).strip()\n",
        "\n",
        "\n",
        "\n",
        "            \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcvcm-NqwvWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#making a new array of cleaned text\n",
        "\n",
        "cleaned_text=data.Text.apply(lambda x :preprocessing(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5HcznXQwx0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_text[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnvpNVGwwzrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cleaning summary\n",
        "cleaned_summary=data.Summary.apply(lambda x :preprocessing(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRfSU69Zw2p9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_summary.replace('',np.nan,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKdjquzzw4XU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_summary[:5]\n",
        "# np.dtype(cleaned_summary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPzBYnnvw6R2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.Summary[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQt69UqOw8BW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining function to add start and end word in summary\n",
        "\n",
        "def addstartend(text):\n",
        "    k='_START_'+str(text)+'_END_'\n",
        "    return k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHlzMgIzw913",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_summary=cleaned_summary.apply(lambda x:addstartend(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn9r4kIZw_r8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_summary[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fESA2V23xCGP",
        "colab_type": "code",
        "outputId": "d5fb3863-df79-43c1-d385-6ff546657de5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "#counting number of words to get padding\n",
        "import matplotlib.pyplot as plt\n",
        "text_word_count=[]\n",
        "for i in cleaned_text:\n",
        "    text_word_count.append(len(i.split()))\n",
        "    \n",
        "summary_word_count=[]\n",
        "for i in cleaned_summary:\n",
        "    summary_word_count.append(len(i.split()))\n",
        "    \n",
        "length_df=pd.DataFrame({'text':text_word_count,'summary':summary_word_count})\n",
        "length_df.hist(bins=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-71d6d39210fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext_word_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcleaned_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtext_word_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cleaned_text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baTrllqHxC9A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09YybAdHxEwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len_text=80\n",
        "max_len_summary=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd-1nhUnxF3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#splitting data for train and validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_tr,x_val,y_tr,y_val=train_test_split(cleaned_text,cleaned_summary,test_size=.1,random_state=0,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sBm0r_AxHub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer()\n",
        "x_tokenizer.fit_on_texts(x_tr)\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "x_tr    =   x_tokenizer.texts_to_sequences(x_tr) \n",
        "x_val   =   x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "#padding zero upto maximum length\n",
        "x_tr    =   pad_sequences(x_tr,  maxlen=max_len_text, padding='post') \n",
        "x_val   =   pad_sequences(x_val, maxlen=max_len_text, padding='post')\n",
        "\n",
        "x_voc_size   =  len(x_tokenizer.word_index) +1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIn2dGHPxKSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preparing a tokenizer for summary on training data \n",
        "y_tokenizer = Tokenizer()\n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "\n",
        "#convert summary sequences into integer sequences\n",
        "y_tr    =   y_tokenizer.texts_to_sequences(y_tr) \n",
        "y_val   =   y_tokenizer.texts_to_sequences(y_val) \n",
        "\n",
        "#padding zero upto maximum length\n",
        "y_tr    =   pad_sequences(y_tr, maxlen=max_len_summary, padding='post')\n",
        "y_val   =   pad_sequences(y_val, maxlen=max_len_summary, padding='post')\n",
        "\n",
        "y_voc_size  =   len(y_tokenizer.word_index) +1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlqbF3DdxNbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "K.clear_session()\n",
        "latent_dim=500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6SyQuEMxPNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#first we will define all the layers of the model than add them \n",
        "\n",
        "######encoder\n",
        "encoder_inputs=Input(shape=(max_len_text,))\n",
        "\n",
        "#embedding\n",
        "enc_emb=Embedding(x_voc_size,latent_dim,trainable=True)(encoder_inputs)\n",
        "\n",
        "#lstm 1\n",
        "encoder_lstm1=LSTM(latent_dim,return_sequences=True,return_state=True)\n",
        "encoder_output1,state_h1,state_c1=encoder_lstm1(enc_emb)\n",
        "\n",
        "#lstm2\n",
        "encoder_lstm2=LSTM(latent_dim,return_sequences=True,return_state=True)\n",
        "encoder_outputs,state_h,state_c=encoder_lstm2(encoder_output1)\n",
        "\n",
        "#####decoder\n",
        "decoder_inputs=Input(shape=(None,))\n",
        "\n",
        "#embedding\n",
        "dec_emb_layer=Embedding(y_voc_size,latent_dim,trainable=True)\n",
        "dec_emb=dec_emb_layer(decoder_inputs)\n",
        "\n",
        "#lstm1\n",
        "decoder_lstm=LSTM(latent_dim,return_sequences=True,return_state=True)\n",
        "decoder_outputs,decoder_fwd_state,decoder_back_state=decoder_lstm(dec_emb,initial_state=[state_h,state_c])\n",
        "\n",
        "#Attention layer\n",
        "attn_layer=AttentionLayer(name='attention_layer')\n",
        "attn_out,attn_state=attn_layer([encoder_outputs,decoder_outputs])\n",
        "\n",
        "#concatenate attention output with decoder output\n",
        "decoder_concat_input=Concatenate(axis=-1,name='concat_layer')([decoder_outputs,attn_out])\n",
        "\n",
        "#dense layer\n",
        "#importnat to study timedistributed layer and its feature\n",
        "\n",
        "decoder_dense=TimeDistributed(Dense(y_voc_size,activation='softmax'))\n",
        "decoder_outputs=decoder_dense(decoder_concat_input)\n",
        "\n",
        "#now summing up the model\n",
        "model=Model([encoder_inputs,decoder_inputs],decoder_outputs)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3mridyzxRQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to create picture of model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3J3MYVkxTmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop',loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DQHix1LxVEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "es=EarlyStopping(monitor='val_loss',mode='min',verbose=1)\n",
        "\n",
        "# es = EarlyStopping(monitor='val_acc', min_delta=.001, patience=5,mode=\"max\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLxiqQ3GxXVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#lets train \n",
        "# %%time\n",
        "# BATCH_SIZE=512\n",
        "# EPOCHS=10\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#  model_checkpoint=ModelCheckpoint('/kaggle/working/bestmodel1.hdf5', monitor='val_loss',verbose=1, savebest_only=True,save_weights_only=False,mode='auto',period=1)\n",
        "modelcheckpoint = ModelCheckpoint(\"/kaggle/working/bestmodel1.h5\", save_best_only=True, verbose=1)\n",
        "# model_checkpoint = ModelCheckpoint(filepath='movie_review_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5',\n",
        "#                                   monitor='val_loss',\n",
        "#                                   verbose=1,\n",
        "#                                   save_best_only=True,\n",
        "#                                   save_weights_only=False,\n",
        "#                                   mode='auto',\n",
        "#                                   period=1)\n",
        "\n",
        "# model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "#           batch_size=batch_size,\n",
        "#           epochs=epochs,\n",
        "#           validation_split=0.2)\n",
        "\n",
        "# model.fit([x_tr,y_tr],y_tr,epochs=30,callbacks=[es],batch_size=512, validation_data=([x_val,y_val],y_val))\n",
        "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=20,callbacks=[es],batch_size=512, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hMfBkarxYKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('/kaggle/working/modelf.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk3X9vZRxa0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from matplotlib import pyplot \n",
        "pyplot.plot(history.history['loss'], label='train') \n",
        "pyplot.plot(history.history['val_loss'], label='test') \n",
        "# pyplot.legend() pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKdxwBLgxc8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37yPRLaExedh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_target_word_index=y_tokenizer.index_word\n",
        "reverse_source_word_index=x_tokenizer.index_word\n",
        "target_word_index=y_tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1eFdXkvxgCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#inference main part of language modelling\n",
        "\n",
        "#encoder\n",
        "encoder_model=Model(inputs=encoder_inputs,outputs=[encoder_outputs,state_h,state_c])\n",
        "\n",
        "#decoder\n",
        "decoder_state_input_h=Input(shape=(latent_dim,))\n",
        "decoder_state_input_c=Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input=Input(shape=(max_len_text,latent_dim))#this is the ouptut of encoder for each text\n",
        "\n",
        "#get the embedding of decoder\n",
        "\n",
        "dec_emb2=dec_emb_layer(decoder_inputs)\n",
        "\n",
        "#to predict the next word \n",
        "decoder_outputs2,state_h2,state_c2=decoder_lstm(dec_emb2,initial_state=[decoder_state_input_h,decoder_state_input_c])\n",
        "\n",
        "#attention layer inference\n",
        "attn_out_inf,attn_states_inf=attn_layer([decoder_hidden_state_input,decoder_outputs2])\n",
        "decoder_inf_concat=Concatenate(axis=-1,name='concat')([decoder_outputs2,attn_out_inf])\n",
        "\n",
        "#dense softmax layer to generate prob distri.\n",
        "decoder_outputs2=decoder_dense(decoder_inf_concat)\n",
        "\n",
        "#final decoder model\n",
        "decoder_model=Model([decoder_inputs]+[decoder_hidden_state_input,decoder_state_input_h,decoder_state_input_c],[decoder_outputs2]+[state_h2,state_c2])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zal26h-jxh_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_target_word_index[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX7Om8WJxjk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defing function for inference\n",
        "# updating the  input of decoder and h and c \n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    #encode the input as state vector\n",
        "    e_out,e_h,e_c=encoder_model.predict(input_seq)\n",
        "    target_seq=np.zeros((1,1))\n",
        "    target_seq[0,0]=target_word_index['start']\n",
        "    \n",
        "    stop_condition=False\n",
        "    decoded_sentence=''\n",
        "    \n",
        "    while not stop_condition:\n",
        "        output_tokens,h,c=decoder_model.predict([target_seq]+[e_out,e_h,e_c])\n",
        "        \n",
        "        #sample a token\n",
        "        sampled_token_index=np.argmax(output_tokens[0,-1,:])\n",
        "        sampled_token=reverse_target_word_index[sampled_token_index]\n",
        "#         print(sampled_token_index)\n",
        "        if (sampled_token!='end'):\n",
        "            decoded_sentence+=' '+sampled_token\n",
        "            \n",
        "            #checking exit condition\n",
        "        if(sampled_token=='end' or len(decoded_sentence.split())>=(max_len_summary-1) ):\n",
        "            stop_condition=True\n",
        "            \n",
        "        #update input word of decoder to previous predict word    \n",
        "        target_seq=np.zeros((1,1))\n",
        "        target_seq[0,0]=sampled_token_index\n",
        "        #update h and c\n",
        "        e_h,e_c=h,c\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn4gbvIrxlWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#now lets make a function to convert all the input text ,summary from int to text\n",
        "\n",
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n",
        "            newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if (i!=0):\n",
        "            newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFqxX4cmxnVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_json=decoder_model.to_json()\n",
        "with open(\"decoder_model.json\",\"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "decoder_model.save_weights(\"decoder_modelf.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGQIJT3wxo5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_json=encoder_model.to_json()\n",
        "with open(\"encoder_model.json\",\"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "encoder_model.save_weights(\"encoder_modelf.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w6k3p0exqi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for i in range(len(x_tr)):\n",
        "  print(\"Review:\",seq2text(x_tr[i]))\n",
        "  print(\"Original summary:\",seq2summary(y_tr[i]))\n",
        "  print(\"Predicted summary:\",decode_sequence(x_val[i].reshape(1,max_len_text)))\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bFy5lSSxZxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}